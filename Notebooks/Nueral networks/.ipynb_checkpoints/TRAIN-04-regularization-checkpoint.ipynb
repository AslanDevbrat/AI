{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "\n",
    "Regularization is a field with lots of ongoing research, so like many other things with deep learning keeping up with the literature matters. That said, we will examine two common tactics for regularlizing neural networks: Dropout and Early Stopping.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout is almost hilariously simple given how well it works: during the training process, randomly throw away the outputs from some percentage of the nodes on a layer-by-layer basis. We change which node's output is thrown away during the training rounds, which forces the network to find multiple possible combinations of nodes that still result in a correct prediction, which makes it less likely that the network has overfit or memorized the training data. \n",
    "\n",
    "![image.png](img/dropout.png)\n",
    "\n",
    "Image from: Srivastava, Nitish, et al. ”Dropout: a simple way to prevent neural networks from\n",
    "overfitting”, JMLR 2014\n",
    "\n",
    "We do have to be somewhat judicious, dropping too much will simply stop the network from learning during training. Dropping too little won't significantly improve the validation/test results. Anothing thing to keep in mind is that dropout will increase the amount of time a model needs to converge. Normally every node gets an adjustment for each batch during training, now only non-dropped nodes get an adjustment. If our dropout rates are quite large, it will take longer for the model to converge. \n",
    "\n",
    "Dropout is easy to implement in Keras, lets take a look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this should all look familiar\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "mnist_num_classes = 10 \n",
    "mnist_image_size = 784\n",
    "\n",
    "(mnist_training_images, mnist_training_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()\n",
    "mnist_training_data = mnist_training_images.reshape(mnist_training_images.shape[0], mnist_image_size) \n",
    "mnist_test_data = mnist_test_images.reshape(mnist_test_images.shape[0], mnist_image_size)\n",
    "\n",
    "mnist_training_labels = to_categorical(mnist_training_labels, mnist_num_classes)\n",
    "mnist_test_labels = to_categorical(mnist_test_labels, mnist_num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helpful function we'll be using all over the place to plot training information:\n",
    "def plot_training_history(history, model):\n",
    "    figure = plt.figure()\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='best')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='best')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    figure.tight_layout()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    loss, accuracy  = model.evaluate(mnist_test_data, mnist_test_labels, verbose=False)\n",
    "\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of the literature suggests using different dropout rates for the hidden layers and the\n",
    "# visible (input) layers. Specifically, people rarely use dropout on the input layer, and if \n",
    "# you do the rate should be very small. Large (.3-.5) dropout rates on the hidden layers are\n",
    "# not uncommon. Lets experiment with different choices for each.\n",
    "\n",
    "# Here, we are using the model that performed pretty well in the previous exercise\n",
    "# and adding dropout layers after each hidden layer. \n",
    "def model_with_dropout(dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=64, activation='relu', input_shape=(mnist_image_size,)))\n",
    "    \n",
    "    for _ in range(5):\n",
    "        model.add(Dense(units=64, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=mnist_num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "\n",
    "dropout_rates = [.8, .5, .3, .2, .1]\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    model = model_with_dropout(dropout_rate)\n",
    "    model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(mnist_training_data, mnist_training_labels, batch_size=batch_size, epochs=epochs, verbose=False, validation_split=validation_split)\n",
    "    print(f'\\ndropout_rate: {dropout_rate}')\n",
    "    plot_training_history(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, using different rates can seriously impact performance.\n",
    "# With very high rates, our networks have not converged, and results are \n",
    "# unstable. At lower rates, training accuracy stays below validation acc.\n",
    "# and thats good. \n",
    "\n",
    "# It's also worth considering dropout only at specific layers\n",
    "# not just at every layer. The complexity that builds up near\n",
    "# the end of a network holds a greater risk of contributing to \n",
    "# overfitting. Experiment with that too:\n",
    "def model_with_selective_dropout(dropout_rate, dropout_layers):\n",
    "    assert dropout_layers <= 5\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=64, activation='relu', input_shape=(mnist_image_size,)))\n",
    "    \n",
    "    no_drop_layers = 5 - dropout_layers\n",
    "    for _ in range(no_drop_layers):\n",
    "        model.add(Dense(units=64, activation='relu'))\n",
    "        \n",
    "    for _ in range(dropout_layers):\n",
    "        model.add(Dense(units=64, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=mnist_num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "\n",
    "# Note, we didn't get far with .8 or .5, so I removed them for this experiment\n",
    "# However, in larger networks .8 and .5 may be reasonable choices! Don't over\n",
    "# generalize the findings that might be specific to this network architecture \n",
    "# and this dataset!\n",
    "dropout_rates = [.3, .2, .1]\n",
    "dropout_layers = [3, 2, 1]\n",
    "\n",
    "for dropout_rate, dropout_layers in product(dropout_rates, dropout_layers):\n",
    "    model = model_with_selective_dropout(dropout_rate, dropout_layers)\n",
    "    model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(mnist_training_data, mnist_training_labels, batch_size=batch_size, epochs=epochs, verbose=False, validation_split=validation_split)\n",
    "    print(f'\\ndropout_rate: {dropout_rate} dropout_layers={dropout_layers}')\n",
    "    plot_training_history(history, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences between these networrs is mostly at the margins, and dropping out on the last layer only seemed to do well—but it does appear that our models start to overfit slightly more with only dropout on the last layer. Lets see if we can tease that out by training for longer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This configuration has the most total dropout, most layers and highest percent\n",
    "# We're giving it 40 epochs now. We hope performance gets a little better!\n",
    "model = model_with_selective_dropout(0.3, 3) \n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(mnist_training_data, mnist_training_labels, batch_size=batch_size, epochs=40, verbose=False, validation_split=validation_split)\n",
    "plot_training_history(history, model)\n",
    "\n",
    "# This model peformed the best on test after 20 epochs, \n",
    "# but validation was worse than training after about 5 epochs\n",
    "# At 40 epochs, we want to see if it starts to overfit too much even with dropout\n",
    "model = model_with_selective_dropout(0.1, 2)\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(mnist_training_data, mnist_training_labels, batch_size=batch_size, epochs=40, verbose=False, validation_split=validation_split)\n",
    "plot_training_history(history, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting how much more stable the network with more dropout was, even though its test performance was worse. But, both of those saw their performance degrade by adding more training rounds! That's a good seguay into the next topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Early stopping is a tactic used in lots of ML models, not just neural networks. As the name suggests, early stopping prevents a model from overfitting by stopping training earlier than specified. Typically, early stopping is tied to a metric (or metrics) like validation loss or validation accuracy—if scores on a validation metric stop improving (or get worse) while training scores continue to rise, we're probably overfitting and we should stop training. \n",
    "\n",
    "There are caveats here, scores can go down in one epoch, then go back up in the next epoch for example. To accomodate this Keras provides a both a `patience` parameter which allows us to specify how many epochs in a row training must not improve before we stop early. Keras also provides as well as a `restore_best_weights` parameter, which allows the network to revert to the best weights found. Say we have `patience=2`, and performance decreases for 2 consecutive rounds, Keras could rever the weights for our model to whatever they were before those two rounds of degrading performance. \n",
    "\n",
    "This has an added benefit of reducing training time in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# There are lots of parameters you can set here, including the amount that counts as a change\n",
    "# what metric to watch, and more. See the documentation for more details. We'll keep it\n",
    "# simple for now, and use the defaults for most of these. \n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Now, lets repeat the same experiment we just did, but with early stopping!\n",
    "model = model_with_selective_dropout(0.3, 3) \n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(mnist_training_data,\n",
    "                    mnist_training_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=40,\n",
    "                    verbose=False,\n",
    "                    validation_split=validation_split,\n",
    "                    callbacks=[early_stopper])\n",
    "\n",
    "plot_training_history(history, model)\n",
    "\n",
    "# And the second model...\n",
    "model = model_with_selective_dropout(0.1, 2)\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(mnist_training_data,\n",
    "                    mnist_training_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=40,\n",
    "                    verbose=False,\n",
    "                    validation_split=validation_split,\n",
    "                    callbacks=[early_stopper])\n",
    "\n",
    "plot_training_history(history, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our test results are essentially the same in both cases, and we saved computational resoures.\n",
    "\n",
    "It is worth noting, we have abused the notion of \"test data\" in this exercise: we're looking at test data for every model we train, making that data (more or less) validation data. If we looked only at the validation & training accuracy/loss curves, we would have good reason to prefer these bottom two situations due to their more stable performance, and lower divergence between training and validation scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
