{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfer learning is, in essence, the application of a neural net trained on one task to a new task. Because convolutional neural nets are almost exclusively used to examine visual data, they are particularly good at transfer learning. \n",
    "\n",
    "A network trained to recognize dogs and cats in photographs will learn some features that are universal to processing photographs, and so it may be a good starting point for a network that detects frogs and birds in photographs. A network trained to detect cancer in radiographs will learn some things that are universal to processing radiographs, and so might be a good starting point for a network that detects foregin bodies, or bone fractures. \n",
    "\n",
    "In this lab we'll use transfer learning to re-train a neural network to perform a new task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should look familiar from the previous labs. \n",
    "# We could choose any of the pre-built nets here instead. \n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "# This is a new built in dataset that we haven't seen before.\n",
    "# It is 60,000 (50k training, 10k test) small (32x32) RGB images \n",
    "# classified into 100 classes:\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "# Constant number of labels, square image shape\n",
    "NUM_CLASSES = 100\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "# FOR DISPLAY PURPOSES\n",
    "unprocessed_training_images = x_train\n",
    "unprocessed_training_labels = y_train\n",
    "\n",
    "# Because the network we're fine-tuning (MobileNetV2) has several pooling layers\n",
    "# the smallest image it can process is 96x96, these images are 32x32. To fix this\n",
    "# we are manually rescaling all the images using scipy. We are also applying the \n",
    "# MobileNetV2 preprocess_input function here. \n",
    "def adjust_input_image(rgb_data):\n",
    "    adjusted = preprocess_input(rgb_data)\n",
    "    \n",
    "    # Scales width and height by 3, leaves color channels at original scale\n",
    "    adjusted = ndimage.zoom(adjusted, (3, 3, 1), order=0)\n",
    "\n",
    "    return adjusted\n",
    "\n",
    "x_train = np.array([adjust_input_image(x) for x in x_train])\n",
    "x_test = np.array([adjust_input_image(x) for x in x_test])\n",
    "\n",
    "# And we still need to one-hot encode the labels as usual\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the preprocessing function changes the pixel values by scaling some of them between -1 and 1, this is not nessesarally typical, and if we use matplotlibs imshow on the preprocessed image-set, it will sometimes look a little funky as a result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0])\n",
    "plt.show()\n",
    "plt.imshow(unprocessed_training_images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also take alook at a few of the examples from cifar100. This is a challenging dataset, the images are small and therefore low resolution, and some are pretty obscure—the 'cloud' and two 'sea' images below lack detail. The 'aquarium fish' and 'rose' and 'squirrel' are tough as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Some of these are genuinely challenging... \n",
    "label_map = [\n",
    "    'apple', # id 0\n",
    "    'aquarium_fish',\n",
    "    'baby',\n",
    "    'bear',\n",
    "    'beaver',\n",
    "    'bed',\n",
    "    'bee',\n",
    "    'beetle',\n",
    "    'bicycle',\n",
    "    'bottle',\n",
    "    'bowl',\n",
    "    'boy',\n",
    "    'bridge',\n",
    "    'bus',\n",
    "    'butterfly',\n",
    "    'camel',\n",
    "    'can',\n",
    "    'castle',\n",
    "    'caterpillar',\n",
    "    'cattle',\n",
    "    'chair',\n",
    "    'chimpanzee',\n",
    "    'clock',\n",
    "    'cloud',\n",
    "    'cockroach',\n",
    "    'couch',\n",
    "    'crab',\n",
    "    'crocodile',\n",
    "    'cup',\n",
    "    'dinosaur',\n",
    "    'dolphin',\n",
    "    'elephant',\n",
    "    'flatfish',\n",
    "    'forest',\n",
    "    'fox',\n",
    "    'girl',\n",
    "    'hamster',\n",
    "    'house',\n",
    "    'kangaroo',\n",
    "    'computer_keyboard',\n",
    "    'lamp',\n",
    "    'lawn_mower',\n",
    "    'leopard',\n",
    "    'lion',\n",
    "    'lizard',\n",
    "    'lobster',\n",
    "    'man',\n",
    "    'maple_tree',\n",
    "    'motorcycle',\n",
    "    'mountain',\n",
    "    'mouse',\n",
    "    'mushroom',\n",
    "    'oak_tree',\n",
    "    'orange',\n",
    "    'orchid',\n",
    "    'otter',\n",
    "    'palm_tree',\n",
    "    'pear',\n",
    "    'pickup_truck',\n",
    "    'pine_tree',\n",
    "    'plain',\n",
    "    'plate',\n",
    "    'poppy',\n",
    "    'porcupine',\n",
    "    'possum',\n",
    "    'rabbit',\n",
    "    'raccoon',\n",
    "    'ray',\n",
    "    'road',\n",
    "    'rocket',\n",
    "    'rose',\n",
    "    'sea',\n",
    "    'seal',\n",
    "    'shark',\n",
    "    'shrew',\n",
    "    'skunk',\n",
    "    'skyscraper',\n",
    "    'snail',\n",
    "    'snake',\n",
    "    'spider',\n",
    "    'squirrel',\n",
    "    'streetcar',\n",
    "    'sunflower',\n",
    "    'sweet_pepper',\n",
    "    'table',\n",
    "    'tank',\n",
    "    'telephone',\n",
    "    'television',\n",
    "    'tiger',\n",
    "    'tractor',\n",
    "    'train',\n",
    "    'trout',\n",
    "    'tulip',\n",
    "    'turtle',\n",
    "    'wardrobe',\n",
    "    'whale',\n",
    "    'willow_tree',\n",
    "    'wolf',\n",
    "    'woman',\n",
    "    'worm'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(25):\n",
    "    ax = plt.subplot(5, 5, i+1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(label_map[unprocessed_training_labels[i][0]])\n",
    "    plt.imshow(unprocessed_training_images[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should look familiar from previous labs.\n",
    "def plot_training_history(history, model):\n",
    "    figure = plt.figure()\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    figure.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly loook a little closer at MobileNetV2, so we can think carefully about which layers to freeze and how to apply transfer learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "# Layers go from 0-154\n",
    "#   initial=0-9\n",
    "#   block1 = 10-18\n",
    "#   block2 = 19-27\n",
    "#   block3 = 28-36\n",
    "#   block4 = 37-45\n",
    "#   block5 = 46-54\n",
    "#   block6 = 55-63\n",
    "#.  block7 = 64-72\n",
    "#   block8 = 73-81\n",
    "#.  block9 = 81-90\n",
    "#   block10= 91-98\n",
    "#   block11= 99-107\n",
    "#.  block12= 108-116\n",
    "#.  block13= 117-125\n",
    "#   block14= 126-134\n",
    "#.  block15= 135-143\n",
    "#.  block16= 144-151\n",
    "#.  final_conv = 152, \n",
    "#   conv_bn 153\n",
    "#   out_relu = 154\n",
    "# for i, layer in enumerate(base_model.layers):\n",
    "#     print(i, layer.name)\n",
    "    \n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Tips and Pitfalls\n",
    "\n",
    "One of the challenges with the Cifar100 dataset is that the images are very small. Roughly 10x smaller than the images that MobileNetV2 was originally trained on. This means 1) there is just less input information, and if we look at some of the Cifar images I'm sure you'll agree that some are quite difficult to identify, even for us humans and 2) the kernals that extracted useful information from larger images might not apply as well to smaller ones. \n",
    "\n",
    "Additionally, different hyperparameters can dramatically impact the effectiveness of transfer learning. Some combinations will be very effective, others will lead to overfitting, others won't learn anything at all. With transfer learning it is important to consider the similarity of the task being trained, as well as the amount of data available. \n",
    "\n",
    "If we have a small amount of data, we'll be more likely to overfit during training. If the task is significantly different than the task the original net trained on, the learned kernels may not extract information that is useful for the new task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first way we'll do transfer learning REALLY saves time, but\n",
    "# only works if you don't want to fine tune any of the layers \n",
    "# from the original network. What we can do instead is memorize\n",
    "# the output of the base network, since it won't be changing,\n",
    "# and then repeatedly train on those outputs and the training labels\n",
    "\n",
    "# Like before, we grab a pretrained model with include_top=False\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "# Unlike before, we're going to just run the images through this base layer once\n",
    "# This takes awhile, we're essentially doing a round of evaluation on both datasets.\n",
    "# And we'll save them incase we want to experiment with different models to transfer\n",
    "# on top of these.\n",
    "training_features = base_model.predict(x_train)\n",
    "np.savez('MobileNetV2_features_train', features=training_features)\n",
    "\n",
    "test_features = base_model.predict(x_test)\n",
    "np.savez('MobileNetV2_features_test', features=test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not run the above cell during this session, uncomment these lines to load the features.\n",
    "# training_features = np.load('MobileNetV2_features_train.npz')['features']\n",
    "# test_features = np.load('MobileNetV2_features_test.npz')['features']\n",
    "\n",
    "# Also unlike before, we're going to build a complete model\n",
    "# in the normal way, but define the input shape based on the \n",
    "# features predicted by base_model!\n",
    "model = Sequential()\n",
    "model.add(GlobalAveragePooling2D(input_shape=training_features.shape[1:]))\n",
    "\n",
    "# Note from the summary, this will result in 1280 nodes, let's use the classic \"squeeze\" and add dropout\n",
    "model.add(Dense(units=640, activation='relu'))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(units=320, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(units=NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(training_features, y_train, batch_size=128, epochs=20, validation_split=0.2, verbose=True)\n",
    "\n",
    "plot_training_history(history, model)\n",
    "loss, accuracy = model.evaluate(test_features, y_test, verbose=False)\n",
    "print(f'Test loss: {loss:.3}')\n",
    "print(f'Test accuracy: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that cifar100 is a very tricky dataset. The images are small, 100 classes is several, and 100 classes amongst rougly 60k examples is only on average 600 samples per class. In fact this 56% test accuracy would have put us in 4th place in a 2016 Kaggle competition. Obviously that's a few years back now... but MobileNetV2 and this 3 layer top isn't exactly \"state of the art\" either. It only took about 3 minutes to train this network. \n",
    "\n",
    "https://www.kaggle.com/c/ml2016-7-cifar-100/leaderboard\n",
    "\n",
    "Boom, we're a top ten Kaggel competitor just like that. Though, looking at validation accuracy and loss, it's not clear we're going to improve much further, and clearly even with significant dropout we're overfitting. It may be worth trying to do transfer learning from a different pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignoring Some Layers and Fine Tuning\n",
    "\n",
    "We can use the same idea, but allow some of the layers of the original network to retrain as well. For example, MobileNetV2 is split into 16 blocks. If we think the final few convolutional layers are too specific to the task our original net was trained on (the ImageNet dataset in this case) then we can chop off some of these blocks and get the trained features from an earlier block, and use THOSE as our training_features. Furthermore, in our new model we can have the same convoluitonal layers as the original mode, and train those. This is called fine-tuning.\n",
    "\n",
    "Unfortunately, doing this the way we have done above is very hard in Keras, especially with merge layers which MobileNetV2 has. Instead, we have to \"freeze\" the layers by setting them to be untrainable manually, then \"train\" the entire network with some layers of the layers frozen, especially the early ones. This is a significant perfomance penalty because we have to recompute the output of all the frozen layers each time, instead of simply memorizing it. \n",
    "\n",
    "The alternative is to manually construct a model that matches the architecture we're seeking, which can be challenging for the more complex architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# One way to do transfer learning is called \"fine tuning\". This is where you take an existing network\n",
    "# and freeze some of the layers. In addition to freezing some layers, you'll have to add at a minimum\n",
    "# a new output layer that matches the number of classes for the new task. \n",
    "\n",
    "# Typically the earlier layers are frozen, because these are likely to have learned universal/simple features.\n",
    "# The later layers have learned combinations of those features and are increasingly specific to the task\n",
    "# that the network was trained for. \n",
    "\n",
    "\n",
    "def transfer_from_mobilenet(optimizer, freeze_first_n_layers, batch_size, epochs):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    \n",
    "    # We'll use the same \"squeeze\" w/ dropout structure from above...\n",
    "    # We have to use the \"functional\" API now, which is why we're not using model.add\n",
    "    old_top = base_model.output\n",
    "    old_top = GlobalAveragePooling2D()(old_top)\n",
    "    old_top = Dense(units=640, activation='relu')(old_top)\n",
    "    old_top = Dropout(rate=0.4)(old_top)\n",
    "    old_top = Dense(units=320, activation='relu')(old_top)\n",
    "    old_top = Dropout(rate=0.2)(old_top)\n",
    "    new_top = Dense(NUM_CLASSES, activation='softmax')(old_top)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=new_top)\n",
    "\n",
    "    for layer in model.layers[:freeze_first_n_layers]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=True)\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os # for saving with paths. \n",
    "\n",
    "# These experiments took *several hours* to run originally.\n",
    "# Saving the model after an experiment is a good idea, then you\n",
    "# can experiment further with promising models, without rerunning\n",
    "# entier experiments.\n",
    "save_directory = 'saved_models'\n",
    "\n",
    "# More agressive optimizers like adam can result in overfitting\n",
    "# We should watch for that in our results.\n",
    "optimizers = [\n",
    "    'sgd',\n",
    "    Adam(lr=0.0001)  # Slower than normal learning rate. \n",
    "]\n",
    "\n",
    "# Note: experimentally freezing only the first 2 blocks\n",
    "# resulted in much slower performance, though for SGD freezing \n",
    "# only the first block did achieve 50% accuracy on test data \n",
    "freeze_layer_counts = [\n",
    "    134,   # Up through block 14 frozen\n",
    "    143    # Up through block 15 frozen\n",
    "    \n",
    "    # If we were to set a value here to 154, it would freeze all the original layers\n",
    "    # which can be quite successful, but we can make training much faster using a \n",
    "    # different tactic to freeze all the original layers as demonstrated above\n",
    "]\n",
    "\n",
    "# This takes a LOOOOONNNNNNNGGGGG time, don't re-run it unless you're prepared to wait. \n",
    "for optimizer, freeze_first_n_layers in product(optimizers, freeze_layer_counts):\n",
    "    saved_name = f'{optimizer}_{freeze_first_n_layers}.h5'\n",
    "    print(f'Transfer learning with {optimizer}, {freeze_first_n_layers}', \"Will save as: \", saved_name)\n",
    "    print()\n",
    "\n",
    "    history, model = transfer_from_mobilenet(optimizer, freeze_first_n_layers, 128, 5)\n",
    "    model.save(os.path.join(save_directory, saved_name))\n",
    "    plot_training_history(history, model)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, model = transfer_from_mobilenet('adam', 151, 128, 5) # Leave the final 2 conv layers alone, freeze all the bottleneck blocks\n",
    "model.save(os.path.join(save_directory, saved_name))\n",
    "plot_training_history(history, model)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(f'Test loss: {loss:.3}')\n",
    "print(f'Test accuracy: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. That's not what we were hoping for was it? \n",
    "\n",
    "**Why do you think this transfer attempt wasn't very successful?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try unfreezing a lot more. We have a reasonable amount of training data, so lets try!\n",
    "\n",
    "# WARNING, THIS TOOK ~2 hours on my MacbookPro\n",
    "history, model = transfer_from_mobilenet('adam', 0, 128, 5) # unfreeze it all!\n",
    "plot_training_history(history, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch — we overfit extremely early and thereafter made very little progress on our validation scores. Brutal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(save_directory, \"adam_train_all.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets slow down the learning rage to try and avoid overfitting so fast.\n",
    "\n",
    "history, model = transfer_from_mobilenet(SGD(learning_rate=0.001), 0, 128, 5) # unfreeze it all!\n",
    "plot_training_history(history, model)\n",
    "model.save(os.path.join(save_directory, \"SDG_train_all_slow.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, that actually looks quite encouraging, if a bit slow... lets train it some more!\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.2, verbose=True)\n",
    "plot_training_history(history, model)\n",
    "model.save(os.path.join(save_directory, \"SDG_train_all_slow_further.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results so Far:\n",
    "\n",
    "Slow and steady wins the race this time around. Based on the curve, and lack of overfitting we could probably continue to train this final model for several more epochs and continue to see improvements. I'll leave that as an exercise for your GPU.\n",
    "\n",
    "Taken together, the above experiments suggest fine tuning isn't working out for these two data sets and MobileNetV2. We got the best results from freezing the whole thing, or retraining the whole thing (but only when we train slowly!!). With fine tuning and a totally frozen pretrained network we ended up overfitting by a lot. \n",
    "\n",
    "This could also be a good time to apply data augmentation to synthetically increase our dataset and make the task harder for the network to memorize. \n",
    "\n",
    "All in all, this is about what we expected. CIFAR and ImageNet really are not that similar, and with CIFAR we do have enough data to justify training the whole network. Starting with pretrained weights has been showen to generally improve the speed of reaching high performance, and this is something else you could easily test in this case with slight modifications to the above code.\n",
    "\n",
    "**We need to experiment with fine tuning at a much lower learning rate with plain SGD to fully validate our hypothesis.** A slow rate worked very will with the full network unfrozen, but it might also work well with fine tuning. On the other hand, the more agressive `adam` performed horribly in both fine tuning and training the whole network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
